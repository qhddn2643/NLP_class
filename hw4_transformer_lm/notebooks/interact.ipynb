{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text with your tranied lagnuage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformer_lm.modeling_transformer import TransformerLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see error `404 Client Error: Not Found` it probably means that \"../output\" is not the correct directory with your tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../output_dir\")\n",
    "model = TransformerLM.from_pretrained(\"../output_dir\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use method `.generate()` to continue your text. This is how it works:\n",
    "\n",
    "1. It accepts a batch of input_ids (batch can contain consist of a single example, but should have the shape [1, sequence_length]).\n",
    "1. The model generates a probability distribution over your vocabulary and selects the token_id with the highest probability.\n",
    "1. This token_id is added to the input_ids and the process repeats untill you reach sequence size = max_length\n",
    "\n",
    "This is a very crude and simple way of generating text. You may notice that the model tends to repeat the same word after a certain point (however, if produces the same word all the time, it is probably not trained perfectly). We will learn more ways of how to generate text given the distribution of the next token later in class.\n",
    "\n",
    "You can find the implementation of `.generate()` in `transformer_lm/modeling_transformer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please keep this example as it is.\n",
    "input_ids = tokenizer(\"The meaning of life is\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated_ids = model.generate(input_ids, max_length=30)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to generate more text.\n",
    "### Play with different prefixes and max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"You can put any text here\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated_ids = model.generate(input_ids, max_length=30)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f86d5ac646110a331c60478f9400a1a64d0cd523be90d887457228f9723636b5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
